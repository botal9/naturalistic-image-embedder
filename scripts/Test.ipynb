{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dangerous-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import ntpath\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from options.test_options import TestOptions\n",
    "import os\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from torchvision import transforms as tf\n",
    "from util import html,util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "south-instruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ../../generator_data/         \n",
      "             dataset_mode: my                            \n",
      "             dataset_root: ../../generator_data/         \n",
      "                direction: AtoB                          \n",
      "             display_port: 9333                          \n",
      "          display_winsize: 256                           \n",
      "       embedding_save_dir: ./checkpoints/emb_vidit4      \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 5                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: my                            \n",
      "               n_layers_D: 3                             \n",
      "                     name: gen_new4                      \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_256                      \n",
      "       new_dataset_option: 2.0                           \n",
      "                      ngf: 64                            \n",
      "                    niter: 80                            \n",
      "              niter_decay: 80                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: True                          \n",
      "                     norm: batch                         \n",
      "                    ntest: inf                           \n",
      "                 num_test: 50                            \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: none                          \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n"
     ]
    }
   ],
   "source": [
    "NAME = 'gen_new4'\n",
    "DS_NAME = ''\n",
    "\n",
    "defaults = {\n",
    "    'dataroot': f'../../generator_data/{DS_NAME}',\n",
    "    'model': 'my',\n",
    "    'dataset_mode': 'my',\n",
    "    'dataset_root': f'../../generator_data/{DS_NAME}',\n",
    "    'embedding_save_dir': './checkpoints/emb_vidit4',\n",
    "    'name': NAME\n",
    "}\n",
    "\n",
    "# defaults = {\n",
    "#     'dataroot': '../../embedding_data/vidit/',\n",
    "#     'model': 'emb',\n",
    "#     'dataset_mode': 'emb',\n",
    "#     'dataset_root': '../../embedding_data/vidit/',\n",
    "# #     'embedding_save_dir': './checkpoints/hdr1/',\n",
    "#     'name': NAME\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "opt = TestOptions(defaults=defaults).parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "approximate-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard-code some parameters for test\n",
    "opt.num_threads = 0   # test code only supports num_threads = 1\n",
    "opt.batch_size = 1    # test code only supports batch_size = 1\n",
    "opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.\n",
    "opt.no_flip = True    # no flip; comment this line if results on flipped images are needed.\n",
    "opt.display_id = -1   # no visdom display; the test code saves the results to a HTML file.\n",
    "\n",
    "# opt.gpu_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intimate-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert opt.isTrain == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "animated-contemporary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading test file\n",
      "dataset [MyDataset] was created\n",
      "The number of training images = 7393\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "loading the model from ./checkpoints/emb_vidit4/150_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 54.415 M\n",
      "-----------------------------------------------\n",
      "model [MyModel] was created\n",
      "loading the model from ./checkpoints/gen_new4/latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 93.475 M\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options\n",
    "\n",
    "dataset_size = len(dataset)    # get the number of images in the dataset.\n",
    "print('The number of training images = %d' % dataset_size)\n",
    "\n",
    "model = create_model(opt)      # create a model given opt.model and other options\n",
    "# model.setup(opt)               # regular setup: load and print networks; create schedulers\n",
    "model.load_networks('latest')\n",
    "model.print_networks(verbose=False)\n",
    "total_iters = 0                # the total number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "meaning-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeInverse(tf.Normalize):\n",
    "    \"\"\"\n",
    "    Undoes the normalization and returns the reconstructed images in the input domain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        mean = torch.as_tensor(mean)\n",
    "        std = torch.as_tensor(std)\n",
    "        std_inv = 1 / (std + 1e-7)\n",
    "        mean_inv = -mean * std_inv\n",
    "        super().__init__(mean=mean_inv, std=std_inv)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return super().__call__(tensor.clone())\n",
    "\n",
    "unnorm = NormalizeInverse((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "unnorm1 = NormalizeInverse((0.5,), (0.5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "streaming-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a website\n",
    "web_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.epoch))  # define the website directory\n",
    "webpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bridal-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rgb(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "def read_rgba(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGBA)\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "def read_bytes(img_path):\n",
    "    with open(img_path, 'rb') as image_file:\n",
    "        return image_file.read()\n",
    "\n",
    "def fread_rgb(img_path):\n",
    "    return (read_rgb(img_path).astype(np.float32) / 255)\n",
    "\n",
    "def read_mask(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "def write_rgb(img, path):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(path, img)\n",
    "\n",
    "def write_rgba(img, path):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGRA)\n",
    "    cv2.imwrite(path, img)\n",
    "\n",
    "def write_mask(img, path):\n",
    "    cv2.imwrite(path, img)\n",
    "    \n",
    "\n",
    "def save(img, path, aspect_ratio=1.0):\n",
    "    if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "        cv2.imwrite(path, img)\n",
    "    else:\n",
    "        write_rgb(img, path)\n",
    "\n",
    "\n",
    "def show(img):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "private-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(tensor):\n",
    "    np_img = torch.squeeze(tensor.detach().to('cpu')).numpy()\n",
    "    np_img = (np.transpose(np_img, (1, 2, 0)) + 1) / 2.0\n",
    "    return (np_img.clip(0, 1) * 255).astype(np.uint8)\n",
    "    \n",
    "def to_img1(tensor):\n",
    "    tensor = tensor.detach().to('cpu')[0]\n",
    "    if unnorm is not None:\n",
    "        tensor = unnorm1(tensor)\n",
    "    np_img = tensor.numpy()\n",
    "    np_img = np_img.transpose((1, 2, 0))\n",
    "    return (np_img.clip(0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "def tensor2im(input_image, imtype=np.uint8):\n",
    "    if not isinstance(input_image, np.ndarray):\n",
    "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
    "            image_tensor = input_image.data\n",
    "        else:\n",
    "            return input_image\n",
    "        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array\n",
    "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
    "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
    "    else:  # if it is a numpy array, do nothing\n",
    "        image_numpy = input_image\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "\n",
    "def display(visuals):\n",
    "    real = to_img(visuals['real'])\n",
    "    comp = to_img(visuals['comp'])\n",
    "    harmonized = to_img(visuals['harmonized'])\n",
    "    \n",
    "    print(f'composition/real/harmonized')\n",
    "    display = np.concatenate([comp, real, harmonized], axis=1)\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(display)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "closing-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(webpage, visuals, image_path, aspect_ratio=1.0, width=256):\n",
    "    \"\"\"Save images to the disk.\n",
    "\n",
    "    Parameters:\n",
    "        webpage (the HTML class) -- the HTML webpage class that stores these imaegs (see html.py for more details)\n",
    "        visuals (OrderedDict)    -- an ordered dictionary that stores (name, images (either tensor or numpy) ) pairs\n",
    "        image_path (str)         -- the string is used to create image paths\n",
    "        aspect_ratio (float)     -- the aspect ratio of saved images\n",
    "        width (int)              -- the images will be resized to width x width\n",
    "\n",
    "    This function will save images stored in 'visuals' to the HTML file specified by 'webpage'.\n",
    "    \"\"\"\n",
    "    image_dir = webpage.get_image_dir()\n",
    "    name = image_path.split('/')[-1]\n",
    "\n",
    "    webpage.add_header(name)\n",
    "    ims, txts, links = [], [], []\n",
    "\n",
    "    for label, im_data in visuals.items():\n",
    "        im = im_data\n",
    "        image_name = '%s_%s.png' % (name, label)\n",
    "        save_path = os.path.join(image_dir, image_name)\n",
    "        save(im, save_path, aspect_ratio=aspect_ratio)\n",
    "        ims.append(image_name)\n",
    "        txts.append(label)\n",
    "        links.append(image_name)\n",
    "    webpage.add_images(ims, txts, links, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "explicit-windsor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing (0000)-th image... ['../../generator_data/HAdobe5k/composite_images/a3630_1_5.jpg']\n",
      "saving (0000)-th image... ['../../generator_data/HAdobe5k/composite_images/a3630_1_5.jpg']\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "\n",
    "if opt.eval:\n",
    "    model.eval()\n",
    "\n",
    "for i, data in enumerate(dataset):\n",
    "    model.set_input(data)  # unpack data from data loader\n",
    "    model.test()           # run inference\n",
    "    visuals = model.get_current_visuals()  # get image results\n",
    "    for label, im_data in visuals.items():\n",
    "        if label=='harmonized':\n",
    "            harmonized = tensor2im(im_data)\n",
    "            img_path = str(data['img_path'])\n",
    "            raw_name = img_path.split('/')[-1]\n",
    "            raw_name = raw_name.replace(('[\\''),'')\n",
    "            raw_name = raw_name.replace(('.jpg\\']'),'.jpg')\n",
    "            image_name = '%s' % raw_name\n",
    "            save_path = os.path.join(opt.results_dir+opt.name+'/test_latest/images/', image_name)\n",
    "            print('processing (%04d)-th image... %s' % (i, img_path))\n",
    "            break\n",
    "            \n",
    "    real = to_img(data['real'])            \n",
    "    comp = to_img(data['comp'])\n",
    "    mask = to_img1(data['mask'])\n",
    "    \n",
    "    # for real images\n",
    "#     mask3 = np.dstack([mask // 255] * 3)\n",
    "#     harmonized = mask3 * harmonized + (1 - mask3) * real\n",
    "    \n",
    "    visuals['real'] = real\n",
    "    visuals['comp'] = comp\n",
    "    visuals['harmonized'] = harmonized\n",
    "    \n",
    "    if i % 1 == 0:  # save images to an HTML file\n",
    "        print('saving (%04d)-th image... %s' % (i, img_path))\n",
    "        save_images(webpage, visuals, img_path, aspect_ratio=opt.aspect_ratio, width=opt.display_winsize)\n",
    "webpage.save()  # save the HTML\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-virtue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
